{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig data_dir=./ not found. Available: ['clean', 'other']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m Speech2TextForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/s2t-small-librispeech-asr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m processor \u001b[39m=\u001b[39m Speech2TextProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/s2t-small-librispeech-asr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mhf-internal-testing/librispeech_asr_demo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdata_dir=./\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mclean\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor(ds[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m], sampling_rate\u001b[39m=\u001b[39mds[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msampling_rate\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_features\u001b[39m\u001b[39m\"\u001b[39m], attention_mask\u001b[39m=\u001b[39minputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/datasets/load.py:1723\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1722\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1724\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1725\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1726\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1727\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1728\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1729\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1730\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1731\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1732\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1733\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1734\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1735\u001b[0m )\n\u001b[1;32m   1737\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/datasets/load.py:1526\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1525\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1527\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1528\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m   1529\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1530\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1531\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1532\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1533\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1534\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1535\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1536\u001b[0m )\n\u001b[1;32m   1538\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/datasets/builder.py:1154\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1154\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1155\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/datasets/builder.py:297\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    298\u001b[0m     config_name,\n\u001b[1;32m    299\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    300\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    303\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/datasets/builder.py:434\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m     builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs\u001b[39m.\u001b[39mget(name)\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m builder_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBUILDER_CONFIGS:\n\u001b[0;32m--> 434\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBuilderConfig \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m not found. Available: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m \u001b[39m# if not using an existing config, then create a new config on the fly with config_kwargs\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m builder_config:\n",
      "\u001b[0;31mValueError\u001b[0m: BuilderConfig data_dir=./ not found. Available: ['clean', 'other']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", data_dir=\"/\")\n",
    "\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", data_dir=\"/\")\n",
    "\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)\n",
    "transcription"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
