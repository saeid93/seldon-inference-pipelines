{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.973744809627533}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment  = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"mamooli weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The task does not provide any default models for options ('es', 'en')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m translator  \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation_es_to_en\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m translator(\u001b[39m\"\u001b[39m\u001b[39mHello, how are you?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:619\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m# Use default model/config/tokenizer for the task if no model is provided\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39m# At that point framework might still be undetermined\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     model, default_revision \u001b[39m=\u001b[39m get_default_model_and_revision(targeted_task, framework, task_options)\n\u001b[1;32m    620\u001b[0m     revision \u001b[39m=\u001b[39m revision \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m default_revision\n\u001b[1;32m    621\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    622\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo model was supplied, defaulted to \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m and revision\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a pipeline without specifying a model name and revision in production is not recommended.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/base.py:374\u001b[0m, in \u001b[0;36mget_default_model_and_revision\u001b[0;34m(targeted_task, framework, task_options)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m task_options:\n\u001b[1;32m    373\u001b[0m     \u001b[39mif\u001b[39;00m task_options \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m defaults:\n\u001b[0;32m--> 374\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe task does not provide any default models for options \u001b[39m\u001b[39m{\u001b[39;00mtask_options\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    375\u001b[0m     default_models \u001b[39m=\u001b[39m defaults[task_options][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    376\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m defaults:\n",
      "\u001b[0;31mValueError\u001b[0m: The task does not provide any default models for options ('es', 'en')"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator  = pipeline(\"translation_es_to_en\")\n",
    "translator(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m summerize  \u001b[39m=\u001b[39m pipeline(task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m summerize(\u001b[39m\"\u001b[39m\u001b[39mIf hearings or trials cannot take place because there are no barristers present to represent defendants, there won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be any trials in which criminals are sent to prison and those who are innocent are acquitted. Victims, like defendants, will be left in limbo, unsure when they will see justice. Tentative plans to broker a deal behind the scenes by bringing forward payments have failed - partly because there is simply no trust between the profession and ministers who wont meet them.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:342\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mUtility factory method to build a :class:`~transformers.Pipeline`.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m    >>> pipeline('ner', model=model, tokenizer=tokenizer)\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m# Retrieve the task\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m targeted_task, task_options \u001b[39m=\u001b[39m check_task(task)\n\u001b[1;32m    344\u001b[0m \u001b[39m# Use default model/config/tokenizer for the task if no model is provided\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[39m# At that point framework might still be undetermined\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:236\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[39mreturn\u001b[39;00m targeted_task, (tokens[\u001b[39m1\u001b[39m], tokens[\u001b[39m3\u001b[39m])\n\u001b[1;32m    234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid translation task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m\u001b[39m format\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, available tasks are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SUPPORTED_TASKS\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summerize  = pipeline(task=\"text-classification\", model='bert-base-uncased')\n",
    "summerize(\"If hearings or trials cannot take place because there are no barristers present to represent defendants, there won't be any trials in which criminals are sent to prison and those who are innocent are acquitted. Victims, like defendants, will be left in limbo, unsure when they will see justice. Tentative plans to broker a deal behind the scenes by bringing forward payments have failed - partly because there is simply no trust between the profession and ministers who wont meet them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m translator  \u001b[39m=\u001b[39m pipeline(task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdinalzein/xlm-roberta-base-finetuned-language-identification\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/seldon-prototype/paper-sum-qa/notebook-version.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m translator(\u001b[39m\"\u001b[39m\u001b[39mHello, how are you?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:342\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mUtility factory method to build a :class:`~transformers.Pipeline`.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m    >>> pipeline('ner', model=model, tokenizer=tokenizer)\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m# Retrieve the task\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m targeted_task, task_options \u001b[39m=\u001b[39m check_task(task)\n\u001b[1;32m    344\u001b[0m \u001b[39m# Use default model/config/tokenizer for the task if no model is provided\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[39m# At that point framework might still be undetermined\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:236\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[39mreturn\u001b[39;00m targeted_task, (tokens[\u001b[39m1\u001b[39m], tokens[\u001b[39m3\u001b[39m])\n\u001b[1;32m    234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid translation task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m\u001b[39m format\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, available tasks are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SUPPORTED_TASKS\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator  = pipeline(task=\"text-classification\", model=\"dinalzein/xlm-roberta-base-finetuned-language-identification\")\n",
    "translator(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration clean-data_dir=%2F\n",
      "Reusing dataset librispeech_asr_demo (/home/cc/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean-data_dir=%2F/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:557: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/generation_utils.py:1777: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['</s> mister quilter is the apostle of the middle classes and we are glad to welcome his gospel</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", data_dir=\"/\")\n",
    "\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
